{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd\n\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport os\nprint(os.listdir(\"../input/previous-kernel-dataset\"))",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['test_clean.csv', 'train_clean.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9624740d14eca30a861c757f7c091a376df37797"
      },
      "cell_type": "markdown",
      "source": "** Objective **\n\nThe objective of this kernel is to experiment with the [Light GBM](https://lightgbm.readthedocs.io/en/latest/index.html) model using  [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization) to find its best hyperparameters. We will be using the dataset we cleaned in a previous [kernel](https://www.kaggle.com/gunbl4d3/exploring-house-data-clean-dataset-for-modelling)."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "** Reading data **\n\nFirst we read the data we processed in the following [kernel](https://www.kaggle.com/gunbl4d3/exploring-house-data-clean-dataset-for-modelling)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9d44deb604962c1118ba2697571e9e6b71bea26"
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/previous-kernel-dataset/train_clean.csv\")\ntest = pd.read_csv(\"../input/previous-kernel-dataset/test_clean.csv\")\n\n#Get ID and features\ny_train = train['SalePrice']\ntrain.drop(columns=['SalePrice', 'Id'], inplace=True)\ntest_id = test['Id']\ntest.drop(columns=['Id'], inplace=True)\n\nprint(\"Does Train feature equal test feature?: \", all(train.columns == test.columns))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Does Train feature equal test feature?:  True\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a08f4de12ccbfa159894b72258f97f56b8950564"
      },
      "cell_type": "markdown",
      "source": "Everything good so far. Since the evaluation metric for this competition is the RMSE on the logarithm of the SalePrice variable, we will convert the target variable by taking its logarithm."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f85ed086c510f7e36c33d4561e6e6dff0e8279a9"
      },
      "cell_type": "code",
      "source": "#Convert target variables to logarithmic scale\ny_train = np.log(y_train)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce6c570f12ab50bd5507c5a89517ffba5b98e351"
      },
      "cell_type": "markdown",
      "source": "** Preparing data fro Light GBM **\n\nLight GBM accepts categorical data without the need to one-hot-encode it. For the model to accept it we need to convert the categorical variables to integer. We will achieve that by casting them to category type and using the category codes as values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79c1ebb8f63f118580738a790d13e348a498fadc"
      },
      "cell_type": "code",
      "source": "#Create LGBM dataset format. Need to convert string categorical variables to int.\ndef categorical_to_int(df):\n    categorical = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n                   'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', \n                   'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', \n                   'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n                   'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', \n                   'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \n                   'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n\n    for col in categorical:\n        df[col] = df[col].astype('category')\n    \n    df[categorical] = df[categorical].apply(lambda x: x.cat.codes)\n    \n    return df\n\nntrain = train.shape[0]\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\nall_data = categorical_to_int(all_data)\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\n\ndtrain = lgb.Dataset(train, label=y_train, free_raw_data=False)\ndtrain.construct()",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "<lightgbm.basic.Dataset at 0x7f9f0cffa240>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "58e39e83e35f6b73b1fd4cd5b530af2fb17c47f8"
      },
      "cell_type": "markdown",
      "source": "** Model building **\n\nWe are going to use the [Light GBM](https://lightgbm.readthedocs.io/en/latest/index.html) model optimized using  [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization) for this submission."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5c34f8789d289842f005051a2f40b89d7d3c637"
      },
      "cell_type": "code",
      "source": "def evaluate_lgbm(max_depth,num_leaves,min_data_in_leaf,eta,feature_fraction):\n    params = {\n        'task': 'train',\n        'objective': 'regression',\n        'categorical_feature': (\"name:MSZoning,Street,Alley,LotShape,LandContour,Utilities,\" \n            \"LotConfig,LandSlope,Neighborhood,Condition1,Condition2,BldgType,HouseStyle,RoofStyle,\"\n            \"RoofMatl,Exterior1st,Exterior2nd,MasVnrType,ExterQual,ExterCond,Foundation,BsmtQual,\"\n            \"BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinType2,Heating,HeatingQC,CentralAir,\"\n            \"Electrical,KitchenQual,Functional,FireplaceQu,GarageType,GarageFinish,GarageQual,\"\n            \"GarageCond,PavedDrive,PoolQC,Fence,MiscFeature,SaleType,SaleCondition\"),\n        'max_depth': int(max_depth),\n        'num_leaves': int(num_leaves),\n        'min_data_in_leaf': int(min_data_in_leaf),\n        'eta': max(eta,0),\n        'feature_fraction': max(min(feature_fraction, 1), 0)\n        }\n    \n    cv_results = lgb.cv(\n        params,\n        dtrain,\n        num_boost_round=1000,\n        nfold=5,\n        metrics='rmse',\n        early_stopping_rounds=10,\n        stratified=False\n        )\n    \n    #Return negative rmse, since bayesian optimization can only maximize\n    return -1.0 * cv_results['rmse-mean'][-1] ",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e157ef34f6293d5f9eb24cdeab2e8c982d402276"
      },
      "cell_type": "code",
      "source": "bayes_optim = BayesianOptimization(evaluate_lgbm, {'max_depth': (1,4),\n                                                   'num_leaves': (2,10),\n                                                   'min_data_in_leaf': (20,100),\n                                                   'eta': (0.001,0.005),\n                                                   'feature_fraction': (0.1,1)})\ngp_params = {'alpha': 1e-5} #For convergence issues\nbayes_optim.maximize(init_points=5, n_iter=25,**gp_params)\n\ncv_params = bayes_optim.res['max']['max_params']\nprint(cv_params)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\u001b[31mInitialization\u001b[0m\n\u001b[94m----------------------------------------------------------------------------------------------------------------\u001b[0m\n Step |   Time |      Value |       eta |   feature_fraction |   max_depth |   min_data_in_leaf |   num_leaves | \n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f605eeba66f600102944c651bcda41f74152824a"
      },
      "cell_type": "markdown",
      "source": "Nice! Now we have a list of tuned parameters for the model. Time to train the final model on all dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "232d2612d04399a14067bde567365c4e86143e0b"
      },
      "cell_type": "code",
      "source": "def submission_prediction(train,y_train,dtrain,test,cv_params):\n    params = {\n        'task': 'train',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'categorical_feature': (\"name:MSZoning,Street,Alley,LotShape,LandContour,Utilities,\" \n            \"LotConfig,LandSlope,Neighborhood,Condition1,Condition2,BldgType,HouseStyle,RoofStyle,\"\n            \"RoofMatl,Exterior1st,Exterior2nd,MasVnrType,ExterQual,ExterCond,Foundation,BsmtQual,\"\n            \"BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinType2,Heating,HeatingQC,CentralAir,\"\n            \"Electrical,KitchenQual,Functional,FireplaceQu,GarageType,GarageFinish,GarageQual,\"\n            \"GarageCond,PavedDrive,PoolQC,Fence,MiscFeature,SaleType,SaleCondition\"),\n        'max_depth': int(cv_params['max_depth']),\n        'num_leaves': int(cv_params['num_leaves']),\n        'min_data_in_leaf': int(cv_params['min_data_in_leaf']),\n        'eta': max(cv_params['eta'],0),\n        'feature_fraction': max(min(cv_params['feature_fraction'], 1), 0)\n        }\n    \n    folds = KFold(n_splits=5, shuffle=True, random_state=0)\n    fold_preds = np.zeros(test.shape[0])\n    oof_preds = np.zeros(train.shape[0])\n\n    for train_idx, valid_idx in folds.split(train):\n        mdl = lgb.train(\n            params=params,\n            train_set=dtrain.subset(train_idx),\n            valid_sets=dtrain.subset(valid_idx),\n            num_boost_round=1000, \n            early_stopping_rounds=10,\n            verbose_eval=50\n        )\n        oof_preds[valid_idx] = mdl.predict(dtrain.data.iloc[valid_idx])\n        fold_preds += mdl.predict(test) / folds.n_splits\n\n        print(\"RMSE on validation set: %.5f\" % \n              np.sqrt(mean_squared_error(y_train.iloc[valid_idx], oof_preds[valid_idx])))\n        \n    return fold_preds",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b70d0a0c79da687811d124de8d4f14ea0c841157"
      },
      "cell_type": "markdown",
      "source": "** Prediction **\n\nNow we can compute the predictions on the test set for the submission. Remember that since we were working on logarithms of the target variable, we have to exponentiate before submitting."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8920beeabf728c17e1e5969c4eb413bb4e0c9f91"
      },
      "cell_type": "code",
      "source": "y_pred = np.exp(submission_prediction(train,y_train,dtrain,test,cv_params))\n\nsubmission = pd.DataFrame({\n    \"Id\": test_id,\n    \"SalePrice\": y_pred,\n    })\n\nsubmission.to_csv('house_prices.csv',index=False),\nsubmission.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e6942412907110043d504ef7fd7cc9c70ac308b4"
      },
      "cell_type": "markdown",
      "source": "** Final notes **\n\n- Try model stacking or ensembling with different models for better predictions."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}